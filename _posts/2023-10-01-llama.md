---
title: "איך מריצים את מודל Llama2 🦙 (גדול) על Colab (קטן)? רמז - קוונטיזציה עם ספריית BitsAndBytes"
author: SemSis 
date: 2023-10-01 00:34:00 +0800
categories: [Llama2, Quantization]
tags: [LLMs]
---

## רקע

משפחת מודלי שפה Llama2 של חברת מטא, כוללת מודלים בגדלים שונים. המודל הבסיסי כולל 70 מיליארד פרמטרים, וכן מודלים קטנים יותר, למשל של כ-13 מיליארד פרמטרים.


## מה המשאבים הנדרשים כדי להריץ מודל כזה? 🔨

מבחינת זיכרון, כדי לטעון את המודל נידרש לזיכרון שמסוגל להחזיק את המכפלה: מספר הפרמטרים של המודל x הגודל של כל פרמטר.
אז בואו נדבר במספרים.
חישוב מהיר: המודל כולל 13 מיליארד פרמטרים, המיוצגים באמצעות 32bit כל אחד.
כלומר, יידרשו כ- 7Billion * 32 = 224 Billion Bits, שהם 28GB.

במערכת Google Colab, יש בערך כ- 15GB~ של RAM, לכן לא נוכל לטעון לזיכרון מודל של 28GB.
לכן, כדי שנוכל בכל זאת להריץ מודל כזה, נצטרך לדחוס אותו איכשהו.

## איך דוחסים מודל כזה? קוונטיזציה
על מנת להקטין את צריכת הזיכרון וזמן הריצה, נדרש להשתמש בטריקים לדחיסת המודל בזכרון. קוונטיזציה היא שיטה לאופטימיזציה של ייצוג בזכרון. במקרה שלנו - נרצה להמיר את הייצוג של פרמטרי המודל מ-32 ביט ל-4.
קוונטיזציה עוזרת בכך שמודל ה- 28 ג׳יגה כעת ניתן לטעון בקלות לזיכרון ה- RAM, וגם מסייע ב- inference time.

## ספריית BitsAndBytes

הספרייה הפייתונית BitAndBytes עוזרת להנגיש LLMs, ו- HuggingFace עשו אינטגרציה איתה. ספריית מעטפת ל- Cuda, 


## איך זה נראה בפייתון?

[1] בשלב הראשון נצטרך לקבל רישיון ל- LLama, אחרי שיש לנו token כזה.


[2] נפתח מחברת ב- google colab ונאפשר GPU.
נתקין את הספריות הנדרשות



```console
!pip install --quiet bitsandbytes
!pip install --quiet git+https://github.com/huggingface/transformers.git
!pip install --quiet accelerate
```

[3] נגדיר את שם המודל

```python
from torch import cuda


model_id = 'meta-llama/Llama-2-13b-chat-hf'
device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'


print(device)
```

[3] כאן החלק המעניין! אנחנו משתמשים ב- BitsAndBytes, ומבקשים ממנו לטעון את המודל כך שכל פרמטר ייצוג באמצעות 4 ביטים במקום 32 או 64. ארבעת הפרמטרים כאן חשובים, נשים לב שהם מציינים:
פרמטר load_in_4bit - מאפשר לנו לטעון ב- 4 ביטים. 

enable 4-bit quantization by replacing the Linear layers with FP4/NF4 layers from
זה סופר חשוב גם מבחינת גודל המודל ב- RAM וגם מהירות inference.  



```python
bnb_config = transformers.BitsAndBytesConfig(
load_in_4bit=True, # 4-bit quantization
bnb_4bit_quant_type='nf4', # Normalized float 4
bnb_4bit_use_double_quant=True, # Second quantization after the first
bnb_4bit_compute_dtype=bfloat16 # Computation type
)
```

פרמטר load_in_4bit - מאפשר לנו לטעון ב- 4 ביטים.
פרמטר bnb_4bit_quant_type ע״י החלפת השכבות הלינאריות (Linear layers) בשכבות מסוג 4-bit NormalFloat, שהוא datatype חדש המיועד לאופטימיזציה של משקלים המתפלגים נורמלית.

זה סופר חשוב גם מבחינת גודל המודל ב- RAM וגם מהירות inference.  


לבדוק להמשך:
איך זה ייראה על CPU? זה אפשרי להרצה ומאוד איטי, או בלתי אפשרי?
למה זה לא עבד על falcon?ֿ
לגבי inference time - איזהשהו גיבוי מספרי ומבוסס ניסויים






## הטייקים של Semantic Sisters

היה claim של מטא שזה מודל open source, אבל בפועל זה לא לגמרי המצב כי …


## רפרנסים

https://colab.research.google.com/drive/1UHS0RS7ksy5UWmtxHPAoUDBAusBc0x4o#scrollTo=S2W3PLS_bmK1
