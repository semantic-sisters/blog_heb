---
title: "איך מריצים את מודל Llama2 🦙 (גדול) על Colab (קטן)? רמז - קוונטיזציה"
author: SemSis 
date: 2023-10-01 00:34:00 +0800
categories: [Llama2, Quantization]
tags: [LLMs]
---

הפעם נספר איך הטכניקה של **קוונטיזציה**, והמימוש שלה בעזרת הספרייה הפייתונית `BitsAndBytes`, מאפשרת את ההרצה של מודל Llama 2 המאסיבי על מחברת Goole Colab קלילה וחינמית 🤯ֿ

## רקע

משפחת מודלי שפה Llama2 של חברת מטא כוללת שלושה וריאנטים עיקריים: המודל הקטן כולל 7 מיליארד פרמטרים, מודל הביניים הכולל 13 מיליארד פרמטרים, ומודל של כ- 70 מיליארד פרמטרים.

## מה המשאבים הנדרשים כדי להריץ מודל כזה? 🔨

מבחינת זיכרון, כדי לטעון את המודל נצטרך כמות זיכרון כזו שהיא לפחות מכסה המכפלה:\
&nbsp;&nbsp;&nbsp;&nbsp; `מספר הפרמטרים של המודל ✖️ הגודל של כל פרמטר`\
אז בואו נדבר במספרים, בחישוב מהיר: המודל כולל 13 מיליארד פרמטרים, שכל אחד מהם מיוצג באמצעות 32 ביט. ונקבל ש-\
&nbsp;&nbsp;&nbsp;&nbsp; `מספר הפרמטרים של המודל ✖️ הגודל של כל פרמטר = 13 מיליארד ✖️ 32 ביטים` \
שהם בסך הכל כ- 28GB.

מערכת Google Colab חינמית כוללת GPU יחיד ובערך כ- 15GB~ של RAM, לכן לא נוכל לטעון לזיכרון מודל של 28GB.\
כדי שנוכל בכל זאת להריץ מודל כזה, נצטרך לצמצם את הגודל שלו איכשהו.\
אז מה עושות?  💡 **דּוֹחֲסוֹת**!

## דחיסה באמצעות קוונטיזציה
על מנת להקטין את צריכת הזיכרון וזמן הריצה של המודל, נרצה להפעיל שיטה לצמצום גודל המודל, אשר אינה מדרדרת באופן מהותי את ביצועיו.\
נוכל להעזר בשיטות קיימות וידועות, כמו pruning ו- distillation, אבל היום נתמקד בקוונטיזציה.\
קוונטיזציה היא טכניקה לצמצום גודל המודל, אשר ממירה את המשקולות של המודל מייצוג ברמת דיוק גבוהה, לנמוכה יותר. במקרה שלנו - נרצה להמיר את הייצוג של פרמטרי המודל מ-32 ביט ל- 4 ביט. כיווץ כזה מאפשר למודלי שפה גדולים להיות נגישים גם בסביבות עם משאבי חישוב מוגבלים.

## ספריית BitsAndBytes

הספרייה הפייתונית `BitAndBytes` היא ספריית מעטפת ל- Cuda, הכוללות מימוש לטכניקות קוונטיזציה שונות. ב- 🤗HuggingFace ביצעו אינטגרציה עם `BitAndBytes`, ובכך מאפשרים למשתמשים להריץ את הרוב הגדול של המודלים הקיימים, באמצעות ייצוג ברמת דיוק של 4 ביט.

## איך זה נראה בפייתון?

[1] בשלב הראשון נצטרך לקבל רישיון ל- LLama, אחרי שיש לנו token כזה.


[2] נפתח מחברת ב- google colab ונאפשר GPU.
נתקין את הספריות הנדרשות



```console
!pip install --quiet bitsandbytes
!pip install --quiet git+https://github.com/huggingface/transformers.git
!pip install --quiet accelerate
```

[3] נגדיר את שם המודל

```python
from torch import cuda


model_id = 'meta-llama/Llama-2-13b-chat-hf'
device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'


print(device)
```

[3] כאן החלק המעניין! אנחנו משתמשים ב- BitsAndBytes, ומבקשים ממנו לטעון את המודל כך שכל פרמטר ייצוג באמצעות 4 ביטים במקום 32 או 64. ארבעת הפרמטרים כאן חשובים, נשים לב שהם מציינים:
פרמטר load_in_4bit - מאפשר לנו לטעון ב- 4 ביטים. 

enable 4-bit quantization by replacing the Linear layers with FP4/NF4 layers from
זה סופר חשוב גם מבחינת גודל המודל ב- RAM וגם מהירות inference.  



```python
bnb_config = transformers.BitsAndBytesConfig(
load_in_4bit=True, # 4-bit quantization
bnb_4bit_quant_type='nf4', # Normalized float 4
bnb_4bit_use_double_quant=True, # Second quantization after the first
bnb_4bit_compute_dtype=bfloat16 # Computation type
)
```

פרמטר load_in_4bit - מאפשר לנו לטעון ב- 4 ביטים.
פרמטר bnb_4bit_quant_type ע״י החלפת השכבות הלינאריות (Linear layers) בשכבות מסוג 4-bit NormalFloat, שהוא datatype חדש המיועד לאופטימיזציה של משקלים המתפלגים נורמלית.

זה סופר חשוב גם מבחינת גודל המודל ב- RAM וגם מהירות inference.  


לבדוק להמשך:
איך זה ייראה על CPU? זה אפשרי להרצה ומאוד איטי, או בלתי אפשרי?
למה זה לא עבד על falcon?ֿ
לגבי inference time - איזהשהו גיבוי מספרי ומבוסס ניסויים






## הטייקים של Semantic Sisters

היה claim של מטא שזה מודל open source, אבל בפועל זה לא לגמרי המצב כי …


## רפרנסים

https://colab.research.google.com/drive/1UHS0RS7ksy5UWmtxHPAoUDBAusBc0x4o#scrollTo=S2W3PLS_bmK1
