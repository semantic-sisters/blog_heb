---
title: "איך מריצים את מודל Llama2 🦙 (גדול) על Colab (קטן)? רמז - קוונטיזציה"
author: SemSis 
date: 2023-10-01 00:34:00 +0800
categories: [Llama2, Quantization]
tags: [LLMs]
---

הפעם נספר איך הטכניקה של **קוונטיזציה**, והמימוש שלה בעזרת הספרייה הפייתונית `BitsAndBytes`, מאפשרת את ההרצה של מודל Llama 2 המאסיבי על מחברת Goole Colab קלילה וחינמית 😱

## רקע

משפחת מודלי שפה Llama2 של חברת מטא כוללת שלושה וריאנטים עיקריים: המודל הקטן כולל 7 מיליארד פרמטרים, מודל הביניים הכולל 13 מיליארד פרמטרים, ומודל של כ- 70 מיליארד פרמטרים.

## מה המשאבים הנדרשים כדי להריץ מודל כזה? 🔨

מבחינת זיכרון, כדי לטעון את המודל נצטרך כמות זיכרון כזו שהיא לפחות מכסה המכפלה:\
&nbsp;&nbsp;&nbsp;&nbsp; `מספר הפרמטרים של המודל ✖️ הגודל של כל פרמטר`\
אז בואו נדבר במספרים, בחישוב מהיר: המודל כולל 13 מיליארד פרמטרים, שכל אחד מהם מיוצג באמצעות 32 ביט. ונקבל ש-\
&nbsp;&nbsp;&nbsp;&nbsp; `מספר הפרמטרים של המודל ✖️ הגודל של כל פרמטר = 13 מיליארד ✖️ 32 ביטים` \
שהם בסך הכל כ- 28GB.

מערכת Google Colab חינמית כוללת GPU יחיד ובערך כ- 15GB~ של RAM, לכן לא נוכל לטעון לזיכרון מודל של 28GB.\
כדי שנוכל בכל זאת להריץ מודל כזה, נצטרך לצמצם את הגודל שלו איכשהו.\
אז מה עושות?  💡 **דּוֹחֲסוֹת**!

## דחיסה באמצעות קוונטיזציה
על מנת להקטין את צריכת הזיכרון וזמן הריצה של המודל, נרצה להפעיל שיטה לצמצום גודל המודל, אשר אינה מדרדרת באופן מהותי את ביצועיו.\
נוכל להעזר בשיטות קיימות וידועות, כמו pruning ו- distillation, אבל היום נתמקד בקוונטיזציה.\
קוונטיזציה היא טכניקה לצמצום גודל המודל, אשר ממירה את המשקולות של המודל מייצוג ברמת דיוק גבוהה, לנמוכה יותר. במקרה שלנו - נרצה להמיר את הייצוג של פרמטרי המודל מ-32 ביט ל- 4 ביט. כיווץ כזה מאפשר למודלי שפה גדולים להיות נגישים גם בסביבות עם משאבי חישוב מוגבלים.

## ספריית BitsAndBytes

הספרייה הפייתונית `BitAndBytes` היא ספריית מעטפת ל- Cuda, הכוללות מימוש לטכניקות קוונטיזציה שונות. ב- 🤗HuggingFace ביצעו אינטגרציה עם `BitAndBytes`, ובכך מאפשרים למשתמשים להריץ את הרוב הגדול של המודלים הקיימים, באמצעות ייצוג ברמת דיוק של 4 ביט.

## איך זה נראה בפייתון?

**[1]** בשלב הראשון נצטרך לקבל רישיון לשימוש למודל Llama2, דרך האתר של HuggingFace. לאחר האישור שלהם תקבלו token גישה.
![My image Name](/assets/images/post02_llama/llama_2_marker_v2.png)

**[2]** נפתח מחברת ב- Goole Colab ונאפשר GPU.
נתקין את הספריות הנדרשות:
```console
!pip install --quiet bitsandbytes
!pip install --quiet git+https://github.com/huggingface/transformers.git
!pip install --quiet accelerate
```

**[3]** נתחבר ל- HuggingFace Hub ונזין כעת את ה- token שקיבלנו בשלב [1] 

```python
from huggingface_hub import notebook_login
notebook_login()
```


**[4]** כאן החלק המעניין! נגדיר את הקונפיגורציה עבור הקוונטיזציה באמצעות `BitsAndBytes`

```python
bnb_config = transformers.BitsAndBytesConfig(
    load_in_4bit=True,               # Enable 4-bit quantization
    bnb_4bit_quant_type='fp4',       # 4bit Floating-point (FP) quantization
    bnb_4bit_use_double_quant=True,  # Enable nested quantization
    bnb_4bit_compute_dtype=bfloat16  # Computation type
)
```

הפרמטרים לקונפיגורציה:
1️⃣ פרמטר `load_in_4bit` - מאפשר לנו לטעון ב- 4 ביטים.
2️⃣ פרמטר `bnb_4bit_quant_type` ע״י החלפת השכבות הלינאריות (Linear layers) בשכבות מסוג 4-bit NormalFloat, שהוא datatype חדש המיועד לאופטימיזציה של משקלים המתפלגים נורמלית.
3️⃣ פרמטר `bnb_4bit_use_double_quant` מאפשר קוונטיזציה מקוננת, לשיפור יעילות השימוש בזיכרון בזמן ה- inference.

לבדוק להמשך:
איך זה ייראה על CPU? זה אפשרי להרצה ומאוד איטי, או בלתי אפשרי?
למה זה לא עבד על falcon?ֿ
לגבי inference time - איזהשהו גיבוי מספרי ומבוסס ניסויים






## הטייקים של Semantic Sisters

היה claim של מטא שזה מודל open source, אבל בפועל זה לא לגמרי המצב כי …


## רפרנסים

https://colab.research.google.com/drive/1UHS0RS7ksy5UWmtxHPAoUDBAusBc0x4o#scrollTo=S2W3PLS_bmK1
